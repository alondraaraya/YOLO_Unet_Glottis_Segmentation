{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db83139-3b31-4d80-843e-ae74bf12e79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 15:12:34.327053: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-18 15:12:34.347728: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-18 15:12:34.347749: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-18 15:12:34.348322: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-18 15:12:34.351783: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-18 15:12:34.778576: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/voicelab/miniconda3/envs/tmpenv/lib/python3.11/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "from tensorflow.keras.models import load_model\n",
    "from utils.masks import get_unet_mask, get_max_yolo_roi, filter_unet_mask_with_yolo\n",
    "from utils.metrics import calculate_iou, calculate_recall, calculate_precision, calculate_dice, calculate_f1, calculate_map50, calculate_mcc, calculate_tnr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a01116-3f1d-4a87-8c4b-5f7b38310903",
   "metadata": {},
   "source": [
    "# Evaluacion UNet tradicional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ae868e-2967-4e4c-ac14-083e2944c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_unet_model(test_dir, unet_model, num_images=3500):\n",
    "    unet_ious = []\n",
    "    unet_recalls = []\n",
    "    unet_precisions = []\n",
    "    unet_dices = []\n",
    "    unet_f1_scores = []\n",
    "    unet_map50s = []\n",
    "    unet_mccs = []\n",
    "    unet_tnrs = []\n",
    "    zero_iou_count = 0\n",
    "    iou_bins = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    iou_histogram = np.zeros(len(iou_bins) - 1)\n",
    "\n",
    "    for idx in range(num_images):\n",
    "        try:\n",
    "            image_name = f\"{idx}.png\"\n",
    "            mask_name = f\"{idx}_seg.png\"\n",
    "            mask_path = os.path.join(test_dir, mask_name)\n",
    "            image_path = os.path.join(test_dir, image_name)\n",
    "\n",
    "            ground_truth_mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            ground_truth_mask = ground_truth_mask.astype(bool)\n",
    "\n",
    "            # Leer la imagen original\n",
    "            image = cv2.imread(image_path)\n",
    "\n",
    "            # Obtener la máscara de UNet y filtrarla con la ROI de YOLO\n",
    "            unet_mask = get_unet_mask(image, unet_model)\n",
    "\n",
    "            # Calcular las métricas\n",
    "            iou_unet = calculate_iou(ground_truth_mask, unet_mask)\n",
    "            recall_unet = calculate_recall(ground_truth_mask, unet_mask)\n",
    "            precision_unet = calculate_precision(ground_truth_mask, unet_mask)\n",
    "            dice_unet = calculate_dice(ground_truth_mask, unet_mask)\n",
    "            f1_unet = calculate_f1(ground_truth_mask, unet_mask)\n",
    "            map50_unet = calculate_map50(ground_truth_mask, unet_mask)\n",
    "            mcc_unet = calculate_mcc(ground_truth_mask, unet_mask)\n",
    "            tnr_unet = calculate_tnr(ground_truth_mask, unet_mask)\n",
    "\n",
    "            if np.isnan(iou_unet):\n",
    "                print(f\"Warning: IoU is NaN for image {image_path}\")\n",
    "            else:\n",
    "                unet_ious.append(iou_unet)\n",
    "                unet_recalls.append(recall_unet)\n",
    "                unet_precisions.append(precision_unet)\n",
    "                unet_dices.append(dice_unet)\n",
    "                unet_f1_scores.append(f1_unet)\n",
    "                unet_map50s.append(map50_unet)\n",
    "                unet_mccs.append(mcc_unet)\n",
    "                unet_tnrs.append(tnr_unet)\n",
    "\n",
    "                if iou_unet == 0.0:\n",
    "                    zero_iou_count += 1\n",
    "                else:\n",
    "                    for i in range(len(iou_bins) - 1):\n",
    "                        if iou_bins[i] <= iou_unet < iou_bins[i + 1]:\n",
    "                            iou_histogram[i] += 1\n",
    "                            break\n",
    "\n",
    "            # Liberar recursos\n",
    "            del ground_truth_mask\n",
    "            del filtered_mask_resized\n",
    "            gc.collect()\n",
    "            tf.keras.backend.clear_session()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Calcular las métricas promedio\n",
    "    avg_iou_unet = np.mean(unet_ious) if unet_ious else 0.0\n",
    "    avg_recall = np.mean(unet_recalls) if unet_recalls else 0.0\n",
    "    avg_precision = np.mean(unet_precisions) if unet_precisions else 0.0\n",
    "    avg_dice = np.mean(unet_dices) if unet_dices else 0.0\n",
    "    avg_f1 = np.mean(unet_f1_scores) if unet_f1_scores else 0.0\n",
    "    avg_map50 = np.mean(unet_map50s) if unet_map50s else 0.0\n",
    "    avg_mcc = np.mean(unet_mccs) if unet_mccs else 0.0\n",
    "    avg_tnr = np.mean(unet_tnrs) if unet_tnrs else 0.0\n",
    "\n",
    "    # Imprimir las métricas promedio\n",
    "    print(f\"Average UNet IoU: {avg_iou_unet}\")\n",
    "    print(f\"Average UNet Recall: {avg_recall}\")\n",
    "    print(f\"Average UNet Precision: {avg_precision}\")\n",
    "    print(f\"Average UNet DICE: {avg_dice}\")\n",
    "    print(f\"Average UNet F1: {avg_f1}\")\n",
    "    print(f\"Average UNet mAP50: {avg_map50}\")\n",
    "    print(f\"Average UNet MCC: {avg_mcc}\")\n",
    "    print(f\"Average UNet TNR: {avg_tnr}\")\n",
    "    print(f\"IoU 0: {zero_iou_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60263cff-a78f-4d7a-bbf2-e4a16266bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0175f5c-4602-498b-96ef-f279c1458f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"/home/voicelab/Desktop/segmentation_glottis/BAGLS/test/test\"\n",
    "yolo_model_path = \"/home/voicelab/Desktop/segmentation_glottis/YOLOV8/best_yolov8n-seg-1cls.pt\"\n",
    "yolo_model = YOLO(yolo_model_path)\n",
    "unet_model_path = \"/home/voicelab/Desktop/segmentation_glottis/metrics/epoch070.h5\"\n",
    "unet_model = load_model(unet_model_path, compile=False, custom_objects={'InstanceNormalization': tfa.layers.InstanceNormalization})\n",
    "evaluate_hybrid_model(test_dir, yolo_model, unet_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daae03e1-c9b5-43bb-b623-27f61c47a124",
   "metadata": {},
   "source": [
    "# Evaluacion YOLOv8 (solo YOLO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea348121-7005-43a6-b96b-1b3287ed4a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "from utils.metrics import calculate_iou, calculate_recall, calculate_precision, calculate_dice, calculate_f1, calculate_map50, calculate_mcc, calculate_tnr\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "def get_yolo_mask(image_path, model):\n",
    "    \"\"\"\n",
    "    Genera una máscara de segmentación utilizando un modelo YOLO.\n",
    "\n",
    "    Parámetros:\n",
    "    image_path (str): Ruta de la imagen de entrada.\n",
    "    model (YOLO): Modelo YOLO preentrenado.\n",
    "\n",
    "    Retorna:\n",
    "    numpy.ndarray: Máscara de segmentación binaria.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        original_shape = image.shape[:2]\n",
    "        results = model(image)\n",
    "\n",
    "        mask = np.zeros(original_shape, dtype=np.uint8)\n",
    "\n",
    "        if results[0].masks is not None:\n",
    "            masks = results[0].masks.xy\n",
    "            for mask_array in masks:\n",
    "                if mask_array.shape[0] == 0:  # Manejar el caso de máscaras vacías\n",
    "                    continue\n",
    "                mask_array = mask_array.astype(np.int32)\n",
    "                cv2.fillPoly(mask, [mask_array], 1)\n",
    "        else:\n",
    "            print(\"No masks found in the results\")\n",
    "\n",
    "        mask = mask.astype(bool)\n",
    "        return mask\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_yolo_mask for image {image_path}: {e}\")\n",
    "        return None\n",
    "def evaluate_yolo_model(test_dir, yolo_model, num_images=3500):\n",
    "    yolo_ious = []\n",
    "    yolo_recalls = []\n",
    "    yolo_precisions = []\n",
    "    yolo_dices = []\n",
    "    yolo_f1_scores = []\n",
    "    yolo_map50s = []\n",
    "    yolo_mccs = []\n",
    "    yolo_tnrs = []\n",
    "\n",
    "    for idx in range(num_images):\n",
    "        try:\n",
    "            image_name = f\"{idx}.png\"\n",
    "            mask_name = f\"{idx}_seg.png\"\n",
    "            mask_path = os.path.join(test_dir, mask_name)\n",
    "            image_path = os.path.join(test_dir, image_name)\n",
    "\n",
    "            ground_truth_mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            ground_truth_mask = ground_truth_mask.astype(bool)\n",
    "\n",
    "            # Leer la imagen original\n",
    "            image = cv2.imread(image_path)\n",
    "\n",
    "            yolo_mask = get_yolo_mask(image_path, yolo_model)\n",
    "            # Calcular métricas usando la máscara de YOLO\n",
    "            iou_yolo = calculate_iou(ground_truth_mask, yolo_mask)\n",
    "            recall_yolo = calculate_recall(ground_truth_mask, yolo_mask)\n",
    "            precision_yolo = calculate_precision(ground_truth_mask, yolo_mask)\n",
    "            dice_yolo = calculate_dice(ground_truth_mask, yolo_mask)\n",
    "            f1_yolo = calculate_f1(ground_truth_mask, yolo_mask)\n",
    "            map50_yolo = calculate_map50(ground_truth_mask, yolo_mask)\n",
    "            mcc_yolo = calculate_mcc(ground_truth_mask, yolo_mask)\n",
    "            tnr_yolo = calculate_tnr(ground_truth_mask, yolo_mask)\n",
    "\n",
    "            yolo_ious.append(iou_yolo)\n",
    "            yolo_recalls.append(recall_yolo)\n",
    "            yolo_precisions.append(precision_yolo)\n",
    "            yolo_dices.append(dice_yolo)\n",
    "            yolo_f1_scores.append(f1_yolo)\n",
    "            yolo_map50s.append(map50_yolo)\n",
    "            yolo_mccs.append(mcc_yolo)\n",
    "            yolo_tnrs.append(tnr_yolo)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Imprimir las métricas promedio\n",
    "    print(\"Average IoU YOLO: \", np.mean(yolo_ious))\n",
    "    print(\"Average Recall YOLO: \", np.mean(yolo_recalls))\n",
    "    print(\"Average Precision YOLO: \", np.mean(yolo_precisions))\n",
    "    print(\"Average DICE YOLO: \", np.mean(yolo_dices))\n",
    "    print(\"Average F1 YOLO: \", np.mean(yolo_f1_scores))\n",
    "    print(\"Average mAP50 YOLO: \", np.mean(yolo_map50s))\n",
    "    print(\"Average MCC YOLO: \", np.mean(yolo_mccs))\n",
    "    print(\"Average TNR YOLO: \", np.mean(yolo_tnrs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a22de2-2add-4557-9ba2-d6a953d100ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"/home/voicelab/Desktop/segmentation_glottis/datasets/BAGLS/test/test\"\n",
    "yolo_model_path = \"/home/voicelab/Desktop/segmentation_glottis/models/YOLO/YOLO11/runs/segment/train3/weights/best.pt\"\n",
    "yolo_model = YOLO(yolo_model_path)\n",
    "evaluate_yolo_model(test_dir, yolo_model, num_images=3500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b331f-b7fb-48e0-8e02-30bca31600ea",
   "metadata": {},
   "source": [
    "# Evaluación del modelo híbrido (UNet + ROI de YOLO)(Filter)\n",
    "Métricas por frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e52934-3f78-4897-8410-8484de7ddc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.masks import get_unet_mask, get_max_yolo_roi, filter_unet_mask_with_yolo\n",
    "from utils.metrics import calculate_iou, calculate_recall, calculate_precision\n",
    "\n",
    "def evaluate_and_plot_hybrid_model(video_path, mask_video_path, yolo_model, unet_model, margin=15):\n",
    "    hybrid_ious = []\n",
    "    hybrid_recalls = []\n",
    "    hybrid_precisions = []\n",
    "    \n",
    "    video_cap = cv2.VideoCapture(video_path)\n",
    "    mask_cap = cv2.VideoCapture(mask_video_path)\n",
    "\n",
    "    roi = get_max_yolo_roi(video_path, yolo_model, margin)\n",
    "    \n",
    "    frame_count = 0\n",
    "    while video_cap.isOpened() and mask_cap.isOpened() and frame_count < 60:\n",
    "        ret_video, frame_video = video_cap.read()\n",
    "        ret_mask, frame_mask = mask_cap.read()\n",
    "\n",
    "        if not ret_video or not ret_mask:\n",
    "            break\n",
    "\n",
    "        ground_truth_mask = frame_mask[:, :, 0] > 127\n",
    "\n",
    "        unet_mask = get_unet_mask(frame_video, unet_model)\n",
    "    \n",
    "        filtered_mask = filter_unet_mask_with_yolo(unet_mask, roi)\n",
    "\n",
    "        filtered_mask_resized = cv2.resize(filtered_mask.astype(np.uint8), (ground_truth_mask.shape[1], ground_truth_mask.shape[0])).astype(bool)\n",
    "\n",
    "        iou_hybrid = calculate_iou(ground_truth_mask, filtered_mask_resized)\n",
    "        recall_hybrid = calculate_recall(ground_truth_mask, filtered_mask_resized)\n",
    "        precision_hybrid = calculate_precision(ground_truth_mask, filtered_mask_resized)\n",
    "\n",
    "        hybrid_ious.append(iou_hybrid)\n",
    "        hybrid_recalls.append(recall_hybrid)\n",
    "        hybrid_precisions.append(precision_hybrid)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        del ground_truth_mask\n",
    "        del filtered_mask_resized\n",
    "        if frame_count % 100 == 0:\n",
    "            gc.collect()\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "    video_cap.release()\n",
    "    mask_cap.release()\n",
    "\n",
    "    plot_metrics(hybrid_ious, hybrid_recalls, hybrid_precisions)\n",
    "\n",
    "def plot_metrics(iou_list, recall_list, precision_list):\n",
    "    frames = range(1, len(iou_list) + 1)  # Números de frame empezando desde 1\n",
    "    \n",
    "    # Ajustar el tamaño de los gráficos para formato IEEE\n",
    "    plt.figure(figsize=(3.45, 6))  # 8.8 cm para una columna, altura proporcional\n",
    "    \n",
    "    # Gráfico de IoU\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(frames, iou_list, label=\"IoU\", color='black', linestyle='-', linewidth=1)\n",
    "    plt.xlabel(\"Frame\", fontsize=8)\n",
    "    plt.ylabel(\"IoU\", fontsize=8)\n",
    "    plt.xticks(np.arange(1, len(iou_list) , 10), fontsize=8)  # Mostrar los números de los frames de 10 en 10\n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.legend(fontsize=8, loc='best')\n",
    "\n",
    "    # Gráfico de Recall\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(frames, recall_list, label=\"Recall\", color='black', linestyle='-', linewidth=1)\n",
    "    plt.xlabel(\"Frame\", fontsize=8)\n",
    "    plt.ylabel(\"Recall\", fontsize=8)\n",
    "    plt.xticks(np.arange(1, len(recall_list) + 1, 10), fontsize=8)  # Ajuste similar para Recall\n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.legend(fontsize=8, loc='best')\n",
    "\n",
    "    # Gráfico de Precision\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(frames, precision_list, label=\"Precision\", color='black', linestyle='-', linewidth=1)\n",
    "    plt.xlabel(\"Frame\", fontsize=8)\n",
    "    plt.ylabel(\"Precision\", fontsize=8)\n",
    "    plt.xticks(np.arange(1, len(precision_list) + 1, 10), fontsize=8)  # Ajuste similar para Precision\n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.legend(fontsize=8, loc='best')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6e8a27-5871-4080-848b-d0b51f585ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_video_path = \"/home/voicelab/Desktop/segmentation_glottis/BAGLS/raw/raw/547.mp4\"\n",
    "mask_video_path = \"/home/voicelab/Desktop/segmentation_glottis/BAGLS/raw/raw/547_seg.mp4\"\n",
    "yolo_model_path = \"/home/voicelab/Desktop/segmentation_glottis/YOLOV8/best_yolov8n-seg-1cls.pt\"\n",
    "yolo_model = YOLO(yolo_model_path)\n",
    "unet_model_path = \"/home/voicelab/Downloads/epoch025.h5\"\n",
    "unet_model = load_model(unet_model_path, compile=False, custom_objects={'InstanceNormalization': tfa.layers.InstanceNormalization})\n",
    "\n",
    "evaluate_and_plot_hybrid_model(original_video_path, mask_video_path, yolo_model, unet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38219e2-f1b7-4f7f-82ea-767caec3de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_video_path = \"/home/voicelab/Desktop/segmentation_glottis/BAGLS/raw/raw/155.mp4\"\n",
    "mask_video_path = \"/home/voicelab/Desktop/segmentation_glottis/BAGLS/raw/raw/155_seg.mp4\"\n",
    "yolo_model_path = \"/home/voicelab/Desktop/segmentation_glottis/YOLOV8/best_yolov8n-seg-1cls.pt\"\n",
    "yolo_model = YOLO(yolo_model_path)\n",
    "unet_model_path = \"/home/voicelab/Downloads/epoch025.h5\"\n",
    "unet_model = load_model(unet_model_path, compile=False, custom_objects={'InstanceNormalization': tfa.layers.InstanceNormalization})\n",
    "\n",
    "evaluate_and_plot_hybrid_model(original_video_path, mask_video_path, yolo_model, unet_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea34030-32fa-47b9-a6a9-41a687f62915",
   "metadata": {},
   "source": [
    "# Comparación UNet vs Híbrido (UNet + YOLO ROI)(Filter)\n",
    "Calcula IoU / Recall / Precision por frame para ambos y grafica la comparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12f0537a-853a-4c60-9479-0419ec1b10ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-20 13:44:48.484679: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-20 13:44:48.504885: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-20 13:44:48.504907: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-20 13:44:48.505575: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-20 13:44:48.509459: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-20 13:44:48.932189: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/voicelab/miniconda3/envs/tmpenv/lib/python3.11/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "from ultralytics import YOLO\n",
    "from utils.metrics import calculate_iou, calculate_recall, calculate_precision\n",
    "from utils.masks import get_unet_mask, get_max_yolo_roi, filter_unet_mask_with_yolo\n",
    "\n",
    "def calculate_metrics_per_frame(original_video_path, mask_video_path, Unet, max_frames=60):\n",
    "    iou_list = []\n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    \n",
    "    original_cap = cv2.VideoCapture(original_video_path)\n",
    "    mask_cap = cv2.VideoCapture(mask_video_path)\n",
    "    \n",
    "    frame_count = 0\n",
    "    while original_cap.isOpened() and mask_cap.isOpened() and frame_count < max_frames:\n",
    "        ret_original, frame_original = original_cap.read()\n",
    "        ret_mask, mask_frame = mask_cap.read()\n",
    "        \n",
    "        if not ret_original or not ret_mask:\n",
    "            break\n",
    "\n",
    "        predicted_mask = get_unet_mask(frame_original, Unet)\n",
    "        ground_truth_mask = mask_frame[:, :, 0] > 127  # Convertir máscara a binario\n",
    "\n",
    "        iou = calculate_iou(ground_truth_mask, predicted_mask)\n",
    "        recall = calculate_recall(ground_truth_mask, predicted_mask)\n",
    "        precision = calculate_precision(ground_truth_mask, predicted_mask)\n",
    "\n",
    "        iou_list.append(iou)\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    original_cap.release()\n",
    "    mask_cap.release()\n",
    "    \n",
    "    return iou_list, recall_list, precision_list\n",
    "\n",
    "\n",
    "def evaluate_and_plot_hybrid_model(video_path, mask_video_path, yolo_model, unet_model, margin=15):\n",
    "    hybrid_ious = []\n",
    "    hybrid_recalls = []\n",
    "    hybrid_precisions = []\n",
    "    \n",
    "    # Abrir el video y el video de máscaras\n",
    "    video_cap = cv2.VideoCapture(video_path)\n",
    "    mask_cap = cv2.VideoCapture(mask_video_path)\n",
    "\n",
    "    roi = get_max_yolo_roi(video_path, yolo_model, margin)\n",
    "    \n",
    "    \n",
    "    frame_count = 0\n",
    "    while video_cap.isOpened() and mask_cap.isOpened() and frame_count < 60:\n",
    "        ret_video, frame_video = video_cap.read()\n",
    "        ret_mask, frame_mask = mask_cap.read()\n",
    "\n",
    "        if not ret_video or not ret_mask:\n",
    "            break\n",
    "\n",
    "        # Convertir el frame de la máscara a binario (blanco es la glotis)\n",
    "        ground_truth_mask = frame_mask[:, :, 0] > 127\n",
    "\n",
    "        # Obtener la máscara de UNet y filtrarla con la ROI de YOLO\n",
    "        unet_mask = get_unet_mask(frame_video, unet_model)\n",
    "    \n",
    "        filtered_mask = filter_unet_mask_with_yolo(unet_mask, roi)\n",
    "\n",
    "        # Redimensionar la máscara filtrada a las dimensiones de la máscara de referencia\n",
    "        filtered_mask_resized = cv2.resize(filtered_mask.astype(np.uint8), (ground_truth_mask.shape[1], ground_truth_mask.shape[0])).astype(bool)\n",
    "\n",
    "        # Calcular las métricas\n",
    "        iou_hybrid = calculate_iou(ground_truth_mask, filtered_mask_resized)\n",
    "        recall_hybrid = calculate_recall(ground_truth_mask, filtered_mask_resized)\n",
    "        precision_hybrid = calculate_precision(ground_truth_mask, filtered_mask_resized)\n",
    "\n",
    "        # Almacenar las métricas\n",
    "        hybrid_ious.append(iou_hybrid)\n",
    "        hybrid_recalls.append(recall_hybrid)\n",
    "        hybrid_precisions.append(precision_hybrid)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        # Liberar recursos\n",
    "        del ground_truth_mask\n",
    "        del filtered_mask_resized\n",
    "        if frame_count % 100 == 0:\n",
    "            gc.collect()\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "    video_cap.release()\n",
    "    mask_cap.release()\n",
    "\n",
    "    return hybrid_ious, hybrid_recalls, hybrid_precisions\n",
    "\n",
    "\n",
    "def plot_combined_metrics(video_path, mask_video_path, yolo_model, unet_model, frames=60):\n",
    "    # Calcular métricas para UNet\n",
    "    iou_unet, recall_unet, precision_unet = calculate_metrics_per_frame(video_path, mask_video_path, unet_model, frames)\n",
    "    \n",
    "    # Calcular métricas para el modelo híbrido\n",
    "    hybrid_ious, hybrid_recalls, hybrid_precisions = evaluate_and_plot_hybrid_model(video_path, mask_video_path, yolo_model, unet_model, margin=15)\n",
    "\n",
    "    # Graficar las métricas combinadas\n",
    "    frame_indices = range(1, len(iou_unet) + 1)\n",
    "\n",
    "    plt.figure(figsize=(6, 8))  # Ajuste del tamaño del gráfico\n",
    "\n",
    "    # Gráfico de IoU\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(frame_indices, iou_unet, label=\"IoU UNet\", color='blue', linestyle='-', linewidth=1)\n",
    "    plt.plot(frame_indices, iou_hybrid, label=\"IoU Híbrido\", color='red', linestyle='--', linewidth=1)\n",
    "    plt.xlabel(\"Frame\", fontsize=10)\n",
    "    plt.ylabel(\"IoU\", fontsize=10)\n",
    "    plt.xticks(np.arange(1, len(frame_indices) + 1, 10), fontsize=8)\n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.legend(fontsize=8, loc='best')\n",
    "\n",
    "    # Gráfico de Recall\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(frame_indices, recall_unet, label=\"Recall UNet\", color='blue', linestyle='-', linewidth=1)\n",
    "    plt.plot(frame_indices, recall_hybrid, label=\"Recall Híbrido\", color='red', linestyle='--', linewidth=1)\n",
    "    plt.xlabel(\"Frame\", fontsize=10)\n",
    "    plt.ylabel(\"Recall\", fontsize=10)\n",
    "    plt.xticks(np.arange(1, len(frame_indices) + 1, 10), fontsize=8)\n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.legend(fontsize=8, loc='best')\n",
    "\n",
    "    # Gráfico de Precision\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(frame_indices, precision_unet, label=\"Precision UNet\", color='blue', linestyle='-', linewidth=1)\n",
    "    plt.plot(frame_indices, precision_hybrid, label=\"Precision Híbrido\", color='red', linestyle='--', linewidth=1)\n",
    "    plt.xlabel(\"Frame\", fontsize=10)\n",
    "    plt.ylabel(\"Precision\", fontsize=10)\n",
    "    plt.xticks(np.arange(1, len(frame_indices) + 1, 10), fontsize=8)\n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.legend(fontsize=8, loc='best')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6531eb6c-3855-4402-8af1-c6964f0fc67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_video_path = \"/home/voicelab/Desktop/segmentation_glottis/BAGLS/raw/raw/155.mp4\"\n",
    "mask_video_path = \"/home/voicelab/Desktop/segmentation_glottis/BAGLS/raw/raw/155_seg.mp4\"\n",
    "yolo_model_path = \"/home/voicelab/Desktop/segmentation_glottis/YOLOV8/best_yolov8n-seg-1cls.pt\"\n",
    "'/home/voicelab/Desktop/segmentation_glottis/datasets/BAGLS/test/test'\n",
    "yolo_model = YOLO(yolo_model_path)\n",
    "unet_model_path = \"/home/voicelab/Downloads/epoch025.h5\"\n",
    "unet_model = load_model(unet_model_path, compile=False, custom_objects={'InstanceNormalization': tfa.layers.InstanceNormalization})\n",
    "plot_combined_metrics(original_video_path, mask_video_path, yolo_model, unet_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19193280-183d-44b7-ae2b-cf27ab315b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
